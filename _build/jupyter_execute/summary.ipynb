{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a964b-a2bc-4d77-b1e8-6928da6b2b73",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this lesson, we have a taken a journey into the world of null hypothesis significance testing. Rather than present NHST as a mechanical set of steps for inference, we have taken the approach of discussing the philosophical questions around statistical inference *in general*, as well as the historical development of NHST. Although we have also presented the facts of NHST in terms of defining the null hypothesis, forming a test-statistic and then calculating a $p$-value, this additional information provides a crucial perspective. Although textbooks often present NHST as a set of agreed-upon steps that provide *the* mechanism for statistical inference, looking more broadly we can see that the reality is anything but. We are already in difficult philosophical territory by engaging with inductive inference. This is made worse with the modern application of NHST, which is an illogical combination of Fisher and Neyman-Pearson. Furthermore, there are many additional problems with NHST, all of which raise doubts about whether this method is even that useful for trying to reach conclusions about a population (as philosophically problematic as this aim may be). Despite all this, the mechanistic (even dogmatic) application of NHST continues. The question is: what can we do we do about it? \n",
    "\n",
    "You may feel, even after reading this lesson, that NHST is *still* a useful approach and are happy to continue using it. In which case, there is no problem. However, if these discussion have unnerved you, or have chimed with any underlying discomfort you already had about the process of NHST, then you may be wondering how we progress from this point? Although switching to Bayesian inference is a possibility, this is not reflective of the current state of Experimental Psychology. Frequentist methods still rule the roost. So how do we make sense of all of this within a Frequentist framework? This will be the focus of this week's synchronous session, where we will be discussing suggestions such as Geoff Cumming's [New Statistics](https://pubmed.ncbi.nlm.nih.gov/24220629/), where we shift the focus of our analyses from *hypothesis testing* to *parameter estimation* and *uncertainty*. However, we also need to accept that, at present, we will be *forced* by journals, editors and other colleagues to use NHST. So remember, even if you want to exorcise $p$-values from your work, you are invariably going to come up against someone who says \"yes, that's all lovely, but is it *significant*?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d42a9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}