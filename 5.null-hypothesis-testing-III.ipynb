{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b3fd6-1753-45e7-a920-fccf68cbdcac",
   "metadata": {},
   "source": [
    "# NHST III: Statistical Significance\n",
    "In the previous parts of this lesson, we established the core mechanics of NHST. During those discussions, we made multiple references to calculating *probabilities* in order to reach conclusions. In this part of the lesson, we will finally formalise how this is achieved. To briefly review, the process of NHST involves the following steps\n",
    "\n",
    "1. Determine a null value for our parameter of interest. This does not have to be 0, but can be any value indicative of *no effect*. \n",
    "2. Compare the null value to the actual value we have calculated and call this difference $\\delta$. \n",
    "3. Convert $\\delta$ into a *standardised* value by dividing by its standard error. The resultant value is called a *test statistic* and expresses our calculated difference in standard error units. \n",
    "4. If we know the standard error, call the test statistic $z$. If we have estimated the standard error from the data, call the test statistic $t$. \n",
    "4. Finally, determine the *null distribution* of the test statistic. For a $z$-statistic, this is a standard normal distribution. For a $t$-statistic, this is a $t$-distribution whose width is governed by the *degrees of freedom*. This allows the distribution to flexibly accommodate how the precision of the estimated standard error influences the range of probable $t$-values.\n",
    "\n",
    "So, by this point, we have a test statistic that captures the deviation from the null in our dataset, and we known the distribution of the test-static when the null is true. Our very last step is to use both of these pieces of information to calculate the *probability* of our observed value of $t$, if the null hypothesis is true. This probability is perhaps one of the most controversial, misunderstood and misused aspects of classical statistical inference. The infamous $p$-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16f292",
   "metadata": {},
   "source": [
    "## The $p$-value and Statistical Significance\n",
    "It is fairly likely ($p < 0.05$) that you will have been exposed to the $p$-value already in your statistical education. If you come from a Psychology background, this is practically guaranteed as the history of scientific enquiry in the field of Psychology is largely the history of amassing $p$-values. As a method of trying to reach conclusions from a statistical model, the $p$-value is ubiquitous. This is largely because, on the face of it, the $p$-value provides an easily applicable and objective criteria for determining evidence against the null hypothesis. However, as we will come to see, the ability of the $p$-value to provide the information that researchers typically want is limited and arguably misleading. Despite the many issues with how $p$-values are used in practise, this humble little probability endures as the metric most commonly employed in Experimental Psychology to reach conclusions. As such, you *have* to understand this approach, even if you disagree with it. We will leave most of the criticisms to one side for the moment and just focus on making sure the definition of the $p$-value is clear. This is critical, as the $p$-value is often misunderstood by students and sometimes (rather worryingly) by researchers as well. As part of this, we will also examine the history of the $p$-value, and NHST in general, because this is illuminating in terms of understanding the role of the $p$-value and also, critically, why its modern useage can appear to be so logically unsatisfying.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc1269",
   "metadata": {},
   "source": [
    "\n",
    "### Defining the $p$-value\n",
    "To begin with, it is useful to understand that the definition of the $p$-value comes directly from Fisher's development of NHST. As we will explore further below, Fisher's original conceptualisation of the $p$-value has been somewhat warped into our modern framework of NHST. So, for the moment, we will focus on Fisher's original concept, before seeing what has happened over the years.\n",
    "\n",
    "The basic definition of the $p$-value is the *probability of obtaining a test-statistic value as extreme, or more, assuming the null hypothsis is true*. Formally, we can write this as\n",
    "\n",
    "$$\n",
    "p = P(t|\\mathcal{H}_{0}),\n",
    "$$\n",
    "\n",
    "where $t$ is our observed test-statistic value. How do we calculate this probability? Well, we know the distribution of $t$ when the null hypothesis is true, as this is just the null $t$-distribution which is centred on 0 and has a width controlled by the degrees of freedom. So, if our hypothesised null value is *correct*, then we would expect the $t$-value calculated over many repeats to have a null $t$-distribution. This capture the degree of natural variation we would expect in the $t$-value across different samples, even when the null hypothesis is correct. If our calculated $t$-value appears *consistent* with this distribution, then it might be that the null is true. However, if our calculated $t$-value appears *inconsistent* with this distribution, then it might be that the null is incorrect.\n",
    "\n",
    "A visualisation of the $p$-value is given in {numref}`pval-fig`. The distribution shown here represents the null distribution of the test statistic. The $p$-value is then the probability of the observed value of the test statistic, which corresponds to the area of the distribution *above* the observed value. This is important, because we are not calculating the probability of seeing *exactly* this calculated value under the null[^probfoot], instead it is the probability of this value *or larger*.\n",
    "\n",
    "```{figure} images/pvalue.png\n",
    "---\n",
    "width: 600px\n",
    "name: pval-fig\n",
    "---\n",
    "Illustration of the p-value as the area under the null distribution of the test statistic above the calculated value.\n",
    "```\n",
    "\n",
    "As an example, say we calculated $t = 1.2$ and have $20$ degrees of freedom. The $p$-value can then be derived in `R` using the `pt()` function (probability from the $t$-distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8cfcd6f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.1220808\n"
     ]
    }
   ],
   "source": [
    "t  <- 1.2\n",
    "df <- 20\n",
    "p  <- pt(q=t, df=df, lower.tail=FALSE)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8a05d",
   "metadata": {},
   "source": [
    "So, the probability of getting a $t$-value of $1.2$ or larger when the null hypothesis is true is $p = 0.122$, or $12.2\\%$. This is only based on the *upper-tail* of the distribution. If we wanted to test the probability of both $t > 1.2$ *and* $t < -1.2$, we would add both tails together by multiplying the $p$-value by 2. This is typically done because we do not always have a strong *directional* hypothesis. In other words, we do not care whether $\\delta$ is positive or negative, only whether the difference is *large*. This is the difference between *one-tailed* and *two-tailed* tests, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5aa16f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " One.tailed Two.tailed\n",
      "  0.1220808  0.2441616\n"
     ]
    }
   ],
   "source": [
    "one.tailed.p  <-     pt(q=t, df=df, lower.tail=FALSE)\n",
    "two.tailed.p  <- 2 * pt(q=t, df=df, lower.tail=FALSE)\n",
    "\n",
    "df <- data.frame(\"One.tailed\"=one.tailed.p,\n",
    "                 \"Two.tailed\"=two.tailed.p)\n",
    "print(df,row.names=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43605a2a",
   "metadata": {},
   "source": [
    "Note that the majority of statistical software will show you two-tailed tests by default because strong directional hypotheses tend to be the *exception* rather than the rule and it is generally safer to report a $p$-value that is *too large* rather than one that is *too small*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8aba4",
   "metadata": {},
   "source": [
    "### Fisher's Concept of the $p$-value\n",
    "The next most obvious question is, once we have calculated the $p$-value, how do we interpret it? For Fisher, the $p$-value was a *continuous measure of evidence against the null*. The smaller the $p$-value, the greater the evidence *against* the null. Or, more precisely, the most *surprising* the data would be, if the null were true. In advocating this perspective of continuous evidence, Fisher provided some heuristics to aid interpretation:\n",
    "\n",
    "```{epigraph}\n",
    "The value for which $p = 0.05$, or 1 in 20, is convenient to use in practice if we want to draw a line beyond which we say that the deviation is significant...\n",
    "\n",
    "-- Fisher, *Statistical Methods for Research Workers* (1925)\n",
    "```\n",
    "\n",
    "So Fisher suggested that we use $p < 0.05$ as a metric for determining *statistical significance*. However, the important point here (which is most readily forgotten in modern NHST) is that this is a *rough guideline* not some sort of *scientific truth*. Indeed, the full quote is much more illuminating\n",
    "\n",
    "```{epigraph}\n",
    "The value for which $p = 0.05$, or 1 in 20, is convenient to use in practice if we want to draw a line beyond which we say that the deviation is significant, but it must not be forgotten that we shall often draw such a line when there is nothing there but chance.\n",
    "\n",
    "-- Fisher, *Statistical Methods for Research Workers* (1925)\n",
    "```\n",
    "\n",
    "So, Fisher viewed this criterion as a convenient heuristic, but fully acknowledged its limitations. A quote from later in his career is also illuminating of Fisher's view \n",
    "\n",
    "```{epigraph}\n",
    "The level of significance in such tests is a matter of convenience, and does not affect the logic of the procedure. The tests of significance...are not to be interpreted rigidly, but are meant to afford evidence, which can be taken into account, together with the rest of the evidence available\n",
    "\n",
    "-- Fisher, *Statistical Methods and Scientific Inference* (1956)\n",
    "```\n",
    "\n",
    "So, again, the criteria of $p < 0.05$ is not a rigid cut-off, but simply an interpretational device that should be used in conjunction with all other available evidence. It is interesting to think about this in light of criticisms of using a threshold of $p < 0.05$ for statistical inference. A more recent famous quote is\n",
    "\n",
    "```{epigraph}\n",
    "Surely, God loves the 0.06 nearly as much as the 0.05?\n",
    "\n",
    "-- Rosnow & Rosenthal (1989)\n",
    "```\n",
    "\n",
    "Indeed, Fisher would probably agree. As a continuous measure of evidence, a $p$-value of 0.06 *is* informative. Although not quite reaching our heuristic of $p < 0.05$, this is still *very close* and suggests some modest evidence against the null. For Fisher, the actual value of $p$ was *very important* for inference. Fisher *never* advocated the position of *rejecting* the null hypothesis when $p < 0.05$. Indeed, Fisher strongly disagreed with this idea, which came from a separate development of NHST attributable to two statisticians called [Jerzy Neyman](https://en.wikipedia.org/wiki/Jerzy_Neyman) and [Egon Pearson](https://en.wikipedia.org/wiki/Egon_Pearson)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370ad6b",
   "metadata": {},
   "source": [
    "## Neyman-Pearson NHST\n",
    "In opposition to Fisher's approach, Neyman-Pearson's approach to NHST saw inference as an exercise in *decision making*. Moreso, they saw inference as the process of making decisions *under uncertainty*, emphasising the long-run control of error rates across repeated experiments. It is here that the concepts of the *alternative hypothesis*, *Type I/Type II errors*, *statistical power* and *rejection of the null* come from. Fisher, in his advocation of the $p$-value as a continuous metric of evidence, strongly disagreed with all these ideas. Neyman-Pearson NHST proceeds in several steps, which we will now briefly explore\n",
    "\n",
    "First, we start by defining *two* competing hypotheses. The null hypothesis $(\\mathcal{H}_{0})$ and the *alternative* hypothesis $(\\mathcal{H}_{1})$. The alternative requires the definition of an *effect size*, which is the magnitude of effect that we expect to see, if the alternative is true. We then define a test-statistic with a known distribution under the null for capturing the discrepancy between the data and the null hypothesis. This is the same procedure as Fisher's approach and so does not need repeating. \n",
    "\n",
    "Next, we need to consider the two different types of error that can occur when we make a decision about the null hypothesis:\n",
    "\n",
    "1. We *reject* $\\mathcal{H}_{0}$ when it is *true*\n",
    "    - This is a Type I error, also known as a *false-positive*, and is denoted $\\alpha$\n",
    "2. We *fail to reject* $\\mathcal{H}_{0}$ when $\\mathcal{H}_{1}$ is *true*\n",
    "    - This is a Type II error, also known as a *false-negative*, and is denoted $\\beta$\n",
    "\n",
    "The idea is that we want to set $\\alpha$ to an acceptable level to minimise false positives across repeat experiments. A typical value is $\\alpha = 0.05$, which no doubt adds to the conflation of Fisher's approach and Neyman-Pearson's approach. We also want to *minimise* $\\beta$ or, equivalently, *maximise* $1 - \\beta$. This is known as *statistical power* and can be interpreted as the probability of correctly rejecting $\\mathcal{H}_{0}$ when $\\mathcal{H}_{1}$ is true. This quantity depends upon the magnitude of the effect size, as well as the sample size and variance.\n",
    "\n",
    "Based on our choice of $\\alpha$ and our test-statistic, we can then create a decision rule. This is achieved using the null distribution of the test-statistic. For instance, if we were using a $t$-statistic with $\\alpha = 0.05$ and 20 degrees of freedom, the critical value is $t_{\\text{crit}} = 1.725$. This can be calculated in `R` using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e05048",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 1.724718\n"
     ]
    }
   ],
   "source": [
    "alpha  <- 0.05\n",
    "df     <- 20\n",
    "t.crit <- qt(p=alpha, df=df, lower.tail=FALSE)\n",
    "\n",
    "print(t.crit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c00a68c",
   "metadata": {},
   "source": [
    "where the `qt()` function will calculate a $t$-value from a probability (whereas `pt()` calculated a probability from a $t$-value). If the null is true, we should only calculate a $t$-value *larger* than $1.725$ around 5% of the time. Thus, over repeated experiments, if the null is true then we will only make a Type I error around 5% of the time. The Type I error rate has therefore been *controlled* at the given $\\alpha$-level. Any value of $t > 1.725$ therefore falls within a *rejection-region*. In other words, a region of values of $t$ which are so large that we will reject the null. \n",
    "\n",
    "Finally, based on the value of $t$ calculated from the data, we either *reject the null hypothesis* when $t > t_{\\text{crit}}$, or *fail to reject the null* when $t < t_{\\text{crit}}$. If we stick rigidly to these rejection regions then then probability of false-positives is controlled at the desired $\\alpha$-level. \n",
    "\n",
    "Notice here that there is no contiuous assessment of the evidence against the null hypothesis, we simply *reject* or *accept* based on a strict threshold. Because of this, the $p$-value has *no role* in thsi procedure. The critical value of the test-statistic is determined *a priori* and we simply make a binary decision depending on what we calculate from the data. The precise control of the error rates requires a black-and-white decision. If our evidence shifts around and our decisions can be flexible, then we lose this control. For Neyman-Pearson, making a decision that controls long-term error *is the point* of statistical inference and a $p$-value is *unnecessary* for this purpose.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799b448",
   "metadata": {},
   "source": [
    "### Confidence Intervals\n",
    "... These were derived *directly* from the rejection regions indicated above. ... The interval contains every parameter value that would lead us to *fail to reject the null hypothesis*, given our current data. If we set our proposed value to anything within this interval, the test-statistic will be below the critical value and we will *fail to reject* the null. So this gives us a range of proposed values where, if we set any of them to the null value, would lead to us failing to reject the null. As such, any proposed value *outside* the interval is far enough away from our estimated value that it would lead to us *rejecting the null*. So, if our null value of interest is *inside* the interval, then we fail to reject the null. If it is *outside* the interval, then we do reject the null. \n",
    "\n",
    "- Proposed value *outside* the interval = reject the null\n",
    "- Proposed value *inside* the interval = fail to reject the null\n",
    "\n",
    "So then we can consider our *null* proposed value. If it is inside the interval then it is close enough to our estimated value and we fail to reject the null. If it is outside the interval then it is far enough away that we will reject the null. In practice, if we use a null value of 0, we end up seeing if 0 is *inside* or *outside* the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a4bfe",
   "metadata": {},
   "source": [
    "## Modern Interpretation of NHST\n",
    "As we can see from the discussions above, the history of NHST is rather tangled and often misunderstood. This is largely because its modern incarnation is a hybrid of both Fisheian and Neyman-Pearsonian NHST. However, these are two *fundamentally different* statistical philosophies. These two frameworks were severly opposed by the other side, with major disagreements on the fundamental purpose of statistical inference and how evidence can be used. These two approaches were never meant to be combined. Indeed, both Fisher and Neyman-Pearson would probably agree that they *cannot* be combined in any meaningful way, due to their philosophical disagreement. And yet, modern NHST *has* merged these different ideas into a confused mishmash that neither side would have endorsed.\n",
    "\n",
    "### Fisher vs Neyman-Pearson\n",
    "Fisher vehemently opposed Neyman-Pearson's approach to NHST. As we saw above, Fisher believed in a continuous assessment of the evidence against the null, using heuristics to *guide* interpretation, rather than creating strict thresholds. Nature does not \"yes\" or \"no, rather it provides evidence that should be interpreted by science. Turning statistical inference into a simple binary decisions makes the process robotic, automated and incapable of any nuance, which goes against how nature actually operates. Fisher emphasised that a null hypothesis might never actually be true in reality. Instead, it is a simplifying assumption. The goal is therefore not to “accept” or “reject” it, but to see how well it can explain the data. In his own words, Fisher said: \n",
    "\n",
    "```{epigraph}\n",
    "It is a mistake to speak of the null hypothesis as though it were a thing we are testing, or as though we were trying to accept or reject it.\n",
    "\n",
    "-- Fisher, *Statistical Methods and Scientific Inference* (1956)\n",
    "```\n",
    "\n",
    "In opposition, Neyman-Pearson were of the view that one cannot sit and ponder the meaning of a $p$-value forever. Eventually, some decision must be reached\n",
    "\n",
    "- \"Is this treatment effective enough to consider offering it to patients on masse?\" \n",
    "- \"Is the quality control of this product good enough to enter production?\" \n",
    "- \"Are the side-effects of this drug severe enough in this one group that the medication should be banned?\"\n",
    "\n",
    "In all these cases a clear decision must be made. For this to happen, we need rules for reaching decisions in the face of uncertainty that *minimise the probability of severe mistakes*. Indeed, Neyman was very critical of the seemingly subjective way a $p$-value could be used to reason about a hypothesis, particularly when a $p$-value is a statement about a *single set of data*\n",
    "\n",
    "```{epigraph}\n",
    "The test of a hypothesis is a rule which tells us whether to reject or not to reject a given hypothesis in the light of observed data. It is not a method of reasoning which we can apply at will to interpret a particular set of data.\n",
    "\n",
    "-- Neyman, *\"Inductive Behavior\" as a Basic Concept of Philosophy of Science* (1957)\n",
    "```\n",
    "\n",
    "Because of this, Neyman-Pearson fundamentally rejected the $p$-value as having any utility in their approach to inference. Because a $p$-value is *data dependant* it goes against the Neyman-Pearson philosophy of defining an error-control procedure that is divorced from the data itself. If we tie the $p$-value to a pre-specified $\\alpha$ (such as $\\alpha = 0.05$), then it can be used as a decision rule, but the *actual value of p does not matter*. At which point, we may as well just use the critical value of the test statistic. The $p$-value adds nothing to this procedure. Neyman-Pearson would likely claim that the $p$-value is just a *post-hoc description of how surprising the data is*. To them, this is just a description of the data, not *inference*. The only reason to calculate a $p$-value is to consider how captible our particular data set is with the null hypothesis. But Neyman-Pearson were not interested in this. For them, statistical inference is simply a yes/no question with no grey areas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cef2f2",
   "metadata": {},
   "source": [
    "### Modern NHST\n",
    "In its most modern incarnation, NHST involves *both* the calculation of $p$-values *and* the use of stringent cut-offs that result in either *rejecting* or *failing to reject* the null. We speak of *statistical power*, *alternative hypotheses* and $\\alpha$*-levels*, yet our statistic software prints $p$-values rather than binary decisions. It is hopefully clear that this modern use of NHST would have horrified Fisher and Neyman-Pearson in equal measure. \n",
    "\n",
    "Fisher would hate to see his $p$-values being used as a decision making tool for rejecting hypotheses, rather than as a measure of evidence. Neyman-Pearson would hate to see statistical software reporting $p$-values, rather than simply stating whether a test passed the critical threshold for rejection. Our modern approach to NHST bleeds decision making and thresholds into Fisher's continuous measures of evidence, and bleeds continuous measures of evidence and the term \"significance\" into Neyman-Pearson's strict decision making approach. We are trying to [have our cake, and eat it](https://en.wikipedia.org/wiki/You_can%27t_have_your_cake_and_eat_it).\n",
    "\n",
    "To see more clearly why the modern mish-mash of these ideas is philosophically inconsistent, consider the situation where we calculate $p = 0.001$:\n",
    "\n",
    "- If we simply *reject* the null because $p < 0.05$ then we are doing Neyman-Pearson NHST with $\\alpha = 0.05$, but using Fisher's $p$-values to do so. There is nothing fundamentally wrong with this, as long as we are only using the $p$-value as a *binary indicator*. However, this begs the question what the point of the $p$-value is? We could reach the same conclusion using the critical value of the test-statistic, which is how Neyman-Pearson designed their approach.\n",
    "- If we interpret this as strong evidence aginst the null, because the $p$-value is quite small, then we are doing Fisher's NHST. We may use a heuristic of $p < 0.05$ as some marker of \"significance\", but this is not a binary rule to accept or reject anything. We simply say that the data is quite surprising, if the null hypothesis were true.\n",
    "- If we mash these two approaches together by rejecting the null hypothesis because $p < 0.05$, but also (whether explicitly or implicitly) take the magnitude of the $p$-value as stronger evidence than another $p$-value in our output table (say $p = 0.048$), then we are doing an incoherent muddling of these two approaches.\n",
    "\n",
    "Although it *feels* like we should be able to combine these methods, this causes problems. For instance, it feels like we should be able to pre-specify a level of error-control, reject the null based on this level and then use the magnitude of the $p$-value to indicate our confidence in that deicison. However, what happens quite often is that researchers will treat values *close* to $p = 0.05$ as *marginally significant*, or move their criteria for significance based on the calculated $p$-values. If we start moving the goalposts based on the $p$-values, then the whole principle of fixing error rates before collecting the data goes out the window. Furthermore, treating $p$-values as *both* a binary indicator and a continuous measure of evidence is inconsistent. For instance, consider calculating $p = 0.06$. Neyman-Pearson would say that you have failed to reject the null hypothesis. It does not matter if $p = 0.06$ or $p = 0.6$, the decision rule is final. Fisher would say that the data are somewhat surpring under the null. Although the value is not below our heuristic of $p < 0.05$, it is suggestive of moderate evidence against the null. This may imply that more data is needed to fully understand what is going on, but there is certainly a hint that the null may not be true.\n",
    "\n",
    "```{figure} images/pvaluepumpkin.jpg\n",
    "---\n",
    "scale: 50%\n",
    "align: left\n",
    "---\n",
    "```\n",
    "\n",
    "Anyone who has used NHST for long enough knows the pains of grappling with a $p$-value that is *nearly significant*. In these situations, the inherent conflict between a strict decision rule and a continuous measure of evidence becomes clear. We see a value that tells us that we cannot reject the null, yet the value is so close to our cutoff that it feels as if it should *mean something*. This is especially true when we get a value like $p = 0.051$ versus $p = 0.96$. In these situations, we want to be Fisher but have been taught to be Neyman-Pearson. We are trying to adhere to a yes/no decision-making framework, yet have been presented with a continuous metric of evidence. If we really want to adhere to the Neyman-Pearson framework, we should remove $p$-value columns from software and replace them with a column that simply says \"reject\" or \"fail to reject\". If we want to adhere to Fisher's framework, we keep $p$-values but remove strict decision rules, as well as concepts of error rates and power. We cannot really have both and remain logically consistent. And yet, here we are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11be775",
   "metadata": {},
   "source": [
    "## NHST in `R`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60324495",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = mpg ~ wt + hp + disp, data = mtcars)\n",
       "\n",
       "Residuals:\n",
       "   Min     1Q Median     3Q    Max \n",
       "-3.891 -1.640 -0.172  1.061  5.861 \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) 37.105505   2.110815  17.579  < 2e-16 ***\n",
       "wt          -3.800891   1.066191  -3.565  0.00133 ** \n",
       "hp          -0.031157   0.011436  -2.724  0.01097 *  \n",
       "disp        -0.000937   0.010350  -0.091  0.92851    \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.639 on 28 degrees of freedom\n",
       "Multiple R-squared:  0.8268,\tAdjusted R-squared:  0.8083 \n",
       "F-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data(mtcars)\n",
    "\n",
    "mod <- lm(mpg ~ wt + hp + disp, data=mtcars)\n",
    "summary(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940dabf",
   "metadata": {},
   "source": [
    "We can also produce confidence intervals using the `confint()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14aa51e6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  2.5 %       97.5 %\n",
      "(Intercept) 32.78169625 41.429314293\n",
      "wt          -5.98488310 -1.616898063\n",
      "hp          -0.05458171 -0.007731388\n",
      "disp        -0.02213750  0.020263482\n"
     ]
    }
   ],
   "source": [
    "print(confint(mod, level=0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da49c3",
   "metadata": {},
   "source": [
    "Notably, other confidence interval levels can be produced. For instance, the intervals we used previous of $\\pm 1 \\times \\text{SE}$ correspond to an approximate 68% CI, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d6d00e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   16 %         84 %\n",
      "(Intercept) 34.96844420 39.242566342\n",
      "wt          -4.88033821 -2.721442954\n",
      "hp          -0.04273454 -0.019578564\n",
      "disp        -0.01141544  0.009541424\n"
     ]
    }
   ],
   "source": [
    "print(confint(mod, level=0.68))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81749d2",
   "metadata": {},
   "source": [
    "Note, however, that we do not need to prescribe to the theory of CIs in order to interpret our parameters in terms of $\\pm 1 \\times \\text{SE}$ or $\\pm 2 \\times \\text{SE}$ or whatever we want. The interpretation remains valid. The part that theory of CIs adds is the *probabilistic* information about the interval. So, the 68% part or the 95% part. These percentages are only valid under the assumptions of CIs. So we can interpret any interval around the estimates we like, however, we need to make further assumptions in order to make any probabilistic claims about the interval's behaviour over reapeated sampling.\n",
    "\n",
    "However, it is not very typical to do this as 95% CIs are considered the standard due to their compatibility with $\\alpha = 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c05833",
   "metadata": {},
   "source": [
    "[^probfoot]: Technically, the probability of a single exact value from a continuous probability distribution is 0. This is not to say it is *impossible*, rather it cannot be calculated because a single point has no area under the distribution curve. It has a height, given by the probability density at that point, but its width is 0.\n",
    "\n",
    "[^alphafoot]: A convention that no-doubt leads to much of the confusion when muddling-up Fisherian NHST and Neyman-Pearsonian NHST.\n",
    "\n",
    "[^crittfoot]: You can calculate this in `R` using `qt(0.05, df=20)`. This goes from $p$-value to $t$-value, whereas `pt()` goes from $t$-value to $p$-value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af937d67",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
